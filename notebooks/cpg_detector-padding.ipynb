{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Sequence\n",
    "from functools import partial\n",
    "import random\n",
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hint we will need following imports\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANGE HERE\n",
    "seed_value = 13\n",
    "random.seed(seed_value)\n",
    "np.random.seed(seed_value)\n",
    "torch.manual_seed(seed_value)\n",
    "torch.cuda.manual_seed(seed_value)\n",
    "\n",
    "# Use this for getting x label\n",
    "def rand_sequence_var_len(n_seqs: int, lb: int=16, ub: int=128) -> Sequence[int]:\n",
    "    for i in range(n_seqs):\n",
    "        seq_len = random.randint(lb, ub)\n",
    "        yield [random.randint(1, 5) for _ in range(seq_len)]\n",
    "\n",
    "\n",
    "# Use this for getting y label\n",
    "def count_cpgs(seq: str) -> int:\n",
    "    cgs = 0\n",
    "    for i in range(0, len(seq) - 1):\n",
    "        dimer = seq[i:i+2]\n",
    "        # note that seq is a string, not a list\n",
    "        if dimer == \"CG\":\n",
    "            cgs += 1\n",
    "    return cgs\n",
    "\n",
    "\n",
    "# Alphabet helpers   \n",
    "alphabet = 'NACGT'\n",
    "dna2int = {a: i for a, i in zip(alphabet, range(1, 6))}\n",
    "int2dna = {i: a for a, i in zip(alphabet, range(1, 6))}\n",
    "dna2int.update({\"pad\": 0})\n",
    "int2dna.update({0: \"pad\"})\n",
    "\n",
    "intseq_to_dnaseq = partial(map, int2dna.get)\n",
    "dnaseq_to_intseq = partial(map, dna2int.get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO complete the task based on the change\n",
    "def prepare_data(num_samples=100, min_len=16, max_len=128):\n",
    "    # TODO prepared the training and test data\n",
    "    # you need to call rand_sequence and count_cpgs here to create the dataset\n",
    "    #step 1\n",
    "    X_dna_seqs_train = list(rand_sequence_var_len(num_samples, min_len, max_len))\n",
    "    #step 2\n",
    "    temp = [list(intseq_to_dnaseq(seq)) for seq in X_dna_seqs_train] # use intseq_to_dnaseq here to convert ids back to DNA seqs\n",
    "    #step3\n",
    "    y_dna_seqs = [count_cpgs(\"\".join(seq)) for seq in temp] # use count_cpgs here to generate labels with temp generated in step2\n",
    "    return X_dna_seqs_train, y_dna_seqs\n",
    "    \n",
    "    \n",
    "min_len, max_len = 64, 128\n",
    "train_x, train_y = prepare_data(2048, min_len, max_len)\n",
    "test_x, test_y = prepare_data(512, min_len, max_len)\n",
    "pop_index = 1386\n",
    "train_x.pop(1386)\n",
    "train_y.pop(1386)\n",
    "\n",
    "classes = np.unique(train_y)\n",
    "num_classes = len(np.unique(train_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some config\n",
    "LSTM_HIDDEN = 64\n",
    "LSTM_LAYER = 1\n",
    "batch_size = 4\n",
    "learning_rate = 1e-3\n",
    "epoch_num = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DnaDataset(Dataset):\n",
    "    def __init__(self, x, y) -> None:\n",
    "        super().__init__()\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.x[index], self.y[index]\n",
    "    \n",
    "def custom_collate(batch):\n",
    "    data, targets = zip(*batch)\n",
    "    targets = [torch.tensor(x) for x in targets]\n",
    "    seqs = [torch.Tensor(x) for x in data]\n",
    "    seqs[0] = nn.ConstantPad1d((0, max_len - seqs[0].shape[0]), 0)(seqs[0])\n",
    "    # Pad sequences dynamically\n",
    "    seqs = nn.utils.rnn.pad_sequence(seqs, batch_first=True, padding_value=0)\n",
    "    return seqs, torch.stack(targets)\n",
    "\n",
    "train_data = DnaDataset(train_x, train_y)\n",
    "train_data_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True, collate_fn=custom_collate)\n",
    "\n",
    "test_data = DnaDataset(test_x, test_y)\n",
    "test_data_loader = DataLoader(test_data, batch_size=batch_size, shuffle=True, collate_fn=custom_collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "class CpGPredictor(torch.nn.Module):\n",
    "    ''' Simple model that uses a LSTM to count the number of CpGs in a sequence '''\n",
    "    def __init__(self):\n",
    "        super(CpGPredictor, self).__init__()\n",
    "        # TODO complete model, you are free to add whatever layers you need here\n",
    "        # We do need a lstm and a classifier layer here but you are free to implement them in your way\n",
    "        self.lstm = nn.LSTM(input_size=128, hidden_size=LSTM_HIDDEN)\n",
    "        # self.fc = nn.Linear(LSTM_HIDDEN, 32)\n",
    "        self.classifier = nn.Linear(LSTM_HIDDEN, num_classes)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # TODO complete forward function\n",
    "        x, _ = self.lstm(x)\n",
    "        logits = self.classifier(x)\n",
    "        logits = self.softmax(logits)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init model / loss function / optimizer etc.\n",
    "model = CpGPredictor()\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs:   40 | Batches per epoch: 512 | Loss: 2.2464367663\n",
      "Epochs:   80 | Batches per epoch: 512 | Loss: 2.1031293173\n",
      "Epochs:  120 | Batches per epoch: 512 | Loss: 1.9948719332\n",
      "Epochs:  160 | Batches per epoch: 512 | Loss: 1.9537221873\n",
      "Epochs:  200 | Batches per epoch: 512 | Loss: 1.9728409878\n",
      "Epochs:  240 | Batches per epoch: 512 | Loss: 1.9305884615\n",
      "Epochs:  280 | Batches per epoch: 512 | Loss: 1.8602278507\n",
      "Epochs:  320 | Batches per epoch: 512 | Loss: 1.7887364433\n",
      "Epochs:  360 | Batches per epoch: 512 | Loss: 1.7753624846\n",
      "Epochs:  400 | Batches per epoch: 512 | Loss: 1.7702160720\n"
     ]
    }
   ],
   "source": [
    "epochs = 400\n",
    "for epoch in range(epochs):\n",
    "  running_loss = 0.0\n",
    "  for i, data in enumerate(train_data_loader):\n",
    "    inputs, labels = data\n",
    "    # labels = torch.argmax(labels, dim=1)\n",
    "    # forward propagation\n",
    "    outputs = model(inputs)\n",
    "    # outputs = torch.tensor([torch.argmax(o) for o in outputs])\n",
    "    loss = loss_fn(outputs, labels)\n",
    "    # set optimizer to zero grad\n",
    "    # to remove previous epoch gradients\n",
    "    optimizer.zero_grad()\n",
    "    # backward propagation\n",
    "    loss.backward()\n",
    "    # optimize\n",
    "    optimizer.step()\n",
    "    running_loss += loss.item()\n",
    "  # display statistics\n",
    "  if not ((epoch + 1) % (epochs // 10)):\n",
    "    print(f'Epochs:{epoch + 1:5d} | ' \\\n",
    "          f'Batches per epoch: {i + 1:3d} | ' \\\n",
    "          f'Loss: {running_loss / (i + 1):.10f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 2.441842555999756\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "res_gs = []\n",
    "res_pred = []\n",
    "\n",
    "with torch.no_grad():\n",
    "  loss = 0\n",
    "  for i, (inputs, labels) in enumerate(test_data_loader):\n",
    "    # calculate output by running through the network\n",
    "    predictions = model(inputs)\n",
    "    res_pred.extend(predictions)\n",
    "    # labels = torch.argmax(labels, dim=1)\n",
    "    res_gs.extend(labels)\n",
    "    loss += loss_fn(predictions, labels)\n",
    "  print(f'Loss: {loss / (i + 1)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Live test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 4\n"
     ]
    }
   ],
   "source": [
    "# sample = list(rand_sequence(128))[0]\n",
    "classes = np.unique(train_y)\n",
    "sample = test_x[10]\n",
    "gt = \"\".join([int2dna.get(i) for i in sample]).count(\"CG\")\n",
    "sample = torch.Tensor(sample)\n",
    "sample = nn.ConstantPad1d((0, max_len - sample.shape[0]), 0)(sample)\n",
    "pred_logit = model(sample.unsqueeze(0))\n",
    "pred = torch.argmax(pred_logit)\n",
    "# conf = float(F.softmax(pred_logit, dim=1)[0][int(pred)]*100)\n",
    "print(gt, classes[pred])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict() ,\"128dim_padding_model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_128_dim = CpGPredictor()\n",
    "model_128_dim.load_state_dict(torch.load(\"128dim_padding_model.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"128_padded_dim_config.json\", \"w\") as f:\n",
    "    f.write(json.dumps(\n",
    "        {\n",
    "            \"alphabet\": alphabet,\n",
    "            \"dna2int\" : dna2int,\n",
    "            \"int2dna\":int2dna,\n",
    "            \"classes\":classes.tolist(),\n",
    "            \"num_classes\": int(num_classes)\n",
    "        }\n",
    "\n",
    "    ))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".multiocr3.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
