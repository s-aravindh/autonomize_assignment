{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G4T6QHHOnfcQ"
   },
   "source": [
    "# Part 1: Build CpG Detector\n",
    "\n",
    "Here we have a simple problem, given a DNA sequence (of N, A, C, G, T), count the number of CpGs in the sequence (consecutive CGs).\n",
    "\n",
    "We have defined a few helper functions / parameters for performing this task.\n",
    "\n",
    "We need you to build a LSTM model and train it to complish this task in PyTorch.\n",
    "\n",
    "A good solution will be a model that can be trained, with high confidence in correctness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "mfS4cLmZD2oB"
   },
   "outputs": [],
   "source": [
    "from typing import Sequence\n",
    "from functools import partial\n",
    "import random\n",
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "_f-brPAvKvTn"
   },
   "outputs": [],
   "source": [
    "# DO NOT CHANGE HERE\n",
    "def set_seed(seed=13):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(13)\n",
    "\n",
    "# Use this for getting x label\n",
    "def rand_sequence(n_seqs: int, seq_len: int=128) -> Sequence[int]:\n",
    "    for i in range(n_seqs):\n",
    "        yield [random.randint(0, 4) for _ in range(seq_len)]\n",
    "\n",
    "# Use this for getting y label\n",
    "def count_cpgs(seq: str) -> int:\n",
    "    cgs = 0\n",
    "    for i in range(0, len(seq) - 1):\n",
    "        dimer = seq[i:i+2]\n",
    "        # note that seq is a string, not a list\n",
    "        if dimer == \"CG\":\n",
    "            cgs += 1\n",
    "    return cgs\n",
    "\n",
    "# Alphabet helpers   \n",
    "alphabet = 'NACGT'\n",
    "dna2int = { a: i for a, i in zip(alphabet, range(5))}\n",
    "int2dna = { i: a for a, i in zip(alphabet, range(5))}\n",
    "\n",
    "intseq_to_dnaseq = partial(map, int2dna.get)\n",
    "dnaseq_to_intseq = partial(map, dna2int.get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 383,
     "status": "ok",
     "timestamp": 1651686469847,
     "user": {
      "displayName": "Ylex",
      "userId": "01820639168093643789"
     },
     "user_tz": 240
    },
    "id": "VK9Qg5GHYxOb",
    "outputId": "0a00bbb6-d9ac-4cf8-ed84-b55b335d7f51"
   },
   "outputs": [],
   "source": [
    "# we prepared two datasets for training and evaluation\n",
    "# training data scale we set to 2048\n",
    "# we test on 512\n",
    "\n",
    "def prepare_data(num_samples=100):\n",
    "    # prepared the training and test data\n",
    "    # you need to call rand_sequence and count_cpgs here to create the dataset\n",
    "    # step 1\n",
    "    X_dna_seqs_train = list(rand_sequence(num_samples))\n",
    "    \"\"\"\n",
    "    hint:\n",
    "        1. You can check X_dna_seqs_train by print, the data is ids which is your training X \n",
    "        2. You first convert ids back to DNA sequence\n",
    "        3. Then you run count_cpgs which will yield CGs counts - this will be the labels (Y)\n",
    "    \"\"\"\n",
    "    #step2\n",
    "    temp = [list(intseq_to_dnaseq(seq)) for seq in X_dna_seqs_train] # use intseq_to_dnaseq here to convert ids back to DNA seqs\n",
    "    #step3\n",
    "    y_dna_seqs = [count_cpgs(\"\".join(seq)) for seq in temp] # use count_cpgs here to generate labels with temp generated in step2\n",
    "    \n",
    "    return X_dna_seqs_train, y_dna_seqs\n",
    "    \n",
    "train_x, train_y = prepare_data(2048)\n",
    "test_x, test_y = prepare_data(512)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "based on train_x and train_y, there are 12 classes lets treat it as a classification problem.\n",
    "this can also be trated as regression problem given the count of CG is the ask. but we will be skipping that for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some config\n",
    "LSTM_HIDDEN = 64 # input is 128 so we change the hidden layer to 64\n",
    "LSTM_LAYER = 1 # we start with one layer only. this is one of hyper-parameter\n",
    "batch_size = 4 # 4 ideal size for my laptop\n",
    "learning_rate = 1e-3 # standard params, also to be treated as hyper-parameter\n",
    "num_classes = len(np.unique(train_y)) # total number of classes\n",
    "classes = np.unique(train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class DnaDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset to fetch x, y based on index\n",
    "    \"\"\"\n",
    "    def __init__(self, x, y) -> None:\n",
    "        super().__init__()\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return torch.Tensor(self.x[index]), torch.tensor(self.y[index])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"128_fixed_dim_config.json\", \"w\") as f:\n",
    "    f.write(json.dumps(\n",
    "        {\n",
    "            \"alphabet\": alphabet,\n",
    "            \"dna2int\" : dna2int,\n",
    "            \"int2dna\":int2dna,\n",
    "            \"classes\":classes.tolist(),\n",
    "            \"num_classes\": int(num_classes)\n",
    "        }\n",
    "\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create data loader\n",
    "\n",
    "# load training data\n",
    "train_data = DnaDataset(train_x, train_y)\n",
    "train_data_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# load test data\n",
    "test_data = DnaDataset(test_x, test_y)\n",
    "test_data_loader = DataLoader(test_data, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "q8fgxrM0LnLy"
   },
   "outputs": [],
   "source": [
    "# Model\n",
    "class CpGPredictor(torch.nn.Module):\n",
    "    ''' Simple model that uses a LSTM to count the number of CpGs in a sequence '''\n",
    "    def __init__(self):\n",
    "        super(CpGPredictor, self).__init__()\n",
    "        # TODO complete model, you are free to add whatever layers you need here\n",
    "        # We do need a lstm and a classifier layer here but you are free to implement them in your way\n",
    "        self.lstm = nn.LSTM(input_size=128, hidden_size=LSTM_HIDDEN)\n",
    "        self.classifier = nn.Linear(LSTM_HIDDEN, num_classes)\n",
    "        self.softmax = nn.Softmax(dim=1) # to get confidence score\n",
    "\n",
    "    def forward(self, x):\n",
    "        # TODO complete forward function\n",
    "        x, _ = self.lstm(x)\n",
    "        logits = self.classifier(x)\n",
    "        logits = self.softmax(logits)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init model / loss function / optimizer etc.\n",
    "model = CpGPredictor()\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs:   40 | Batches per epoch: 512 | Loss: 2.2646126824\n",
      "Epochs:   80 | Batches per epoch: 512 | Loss: 2.0689358509\n",
      "Epochs:  120 | Batches per epoch: 512 | Loss: 1.9184411524\n",
      "Epochs:  160 | Batches per epoch: 512 | Loss: 1.9133717888\n",
      "Epochs:  200 | Batches per epoch: 512 | Loss: 1.9110925191\n",
      "Epochs:  240 | Batches per epoch: 512 | Loss: 1.8466680693\n",
      "Epochs:  280 | Batches per epoch: 512 | Loss: 1.8391595234\n",
      "Epochs:  320 | Batches per epoch: 512 | Loss: 1.7844871006\n",
      "Epochs:  360 | Batches per epoch: 512 | Loss: 1.7738771003\n",
      "Epochs:  400 | Batches per epoch: 512 | Loss: 1.7723775983\n"
     ]
    }
   ],
   "source": [
    "epochs = 400\n",
    "for epoch in range(epochs):\n",
    "  running_loss = 0.0\n",
    "  for i, data in enumerate(train_data_loader):\n",
    "    inputs, labels = data\n",
    "    # labels = torch.argmax(labels, dim=1)\n",
    "    # forward propagation\n",
    "    outputs = model(inputs)\n",
    "    loss = loss_fn(outputs, labels)\n",
    "    # set optimizer to zero grad\n",
    "    # to remove previous epoch gradients\n",
    "    optimizer.zero_grad()\n",
    "    # backward propagation\n",
    "    loss.backward()\n",
    "    # optimize\n",
    "    optimizer.step()\n",
    "    running_loss += loss.item()\n",
    "  # display statistics\n",
    "  if not ((epoch + 1) % (epochs // 10)):\n",
    "    print(f'Epochs:{epoch + 1:5d} | ' \\\n",
    "          f'Batches per epoch: {i + 1:3d} | ' \\\n",
    "          f'Loss: {running_loss / (i + 1):.10f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 2.5340142250061035\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "res_gs = []\n",
    "res_pred = []\n",
    "\n",
    "with torch.no_grad():\n",
    "  loss = 0\n",
    "  for i, (inputs, labels) in enumerate(test_data_loader):\n",
    "    # calculate output by running through the network\n",
    "    predictions = model(inputs)\n",
    "    res_pred.extend(predictions)\n",
    "    # labels = torch.argmax(labels, dim=1)\n",
    "    res_gs.extend(labels)\n",
    "    loss += loss_fn(predictions, labels)\n",
    "  print(f'Loss: {loss / (i + 1)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Live test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7 5\n"
     ]
    }
   ],
   "source": [
    "# sample = list(rand_sequence(128))[0]\n",
    "sample = test_x[10]\n",
    "gt = \"\".join([int2dna.get(i) for i in sample]).count(\"CG\")\n",
    "pred_logit = model(torch.Tensor(sample).unsqueeze(0))\n",
    "pred = torch.argmax(pred_logit)\n",
    "# conf = float(F.softmax(pred_logit, dim=1)[0][int(pred)]*100)\n",
    "print(gt, classes[pred])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict() ,\"128dim_model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_128_dim = CpGPredictor()\n",
    "model_128_dim.load_state_dict(torch.load(\"128dim_model.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3, 3, 3, 0, 2, 3, 2, 2, 2, 0, 2, 4, 2, 4, 4, 1, 3, 3, 3, 3, 3, 1, 1, 0, 0, 2, 1, 4, 4, 4, 0, 3, 1, 2, 4, 3, 0, 3, 4, 0, 2, 3, 4, 0, 4, 3, 2, 1, 1, 1, 4, 1, 2, 4, 3, 0, 1, 0, 0, 4, 3, 2, 2, 3, 4, 3, 4, 1, 1, 4, 4, 1, 4, 0, 0, 2, 3, 0, 4, 1, 2, 4, 3, 4, 4, 0, 0, 3, 2, 0, 2, 2, 1, 2, 0, 3, 2, 2, 2, 1, 3, 0, 1, 3, 0, 4, 3, 1, 3, 0, 0, 0, 3, 0, 4, 0, 1, 3, 2, 2, 1, 1, 2, 2, 1, 0, 3, 3]\n"
     ]
    }
   ],
   "source": [
    "print(test_x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(\"\".join([int2dna.get(i) for i in test_x[0]]))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Xi Yangs Copy of broken-nn-template.ipynb",
   "provenance": [
    {
     "file_id": "13GlbI_pdKNES8I718iwl1KNnMZ73iOOn",
     "timestamp": 1651680757732
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
